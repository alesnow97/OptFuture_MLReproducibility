Module path:  Environments.NS_Reco NS_Reco
Dynamically loaded from:  <class 'Environments.NS_Reco.NS_Reco'>
Reward Amplitudes: [4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01
 1.46755891e-01] :: Avg 0.317309867282206 
Module path:  Src.Algorithms.ProOLS ProOLS
Dynamically loaded from:  <class 'Src.Algorithms.ProOLS.ProOLS'>
=====Configurations=====
 Namespace(NN_basis_dim='32', Policy_basis_dim='32', actor_lr=0.0013967975033711264, algo_name='ProOLS', base=0, batch_size=1000, buffer_size=1000, debug=True, delta=1, entropy_lambda=0.009309067356364342, env_name='NS_Reco', experiment='NS_0d_', extrapolator_basis='Fourier', folder_suffix='Default', fourier_coupled=True, fourier_k=5, fourier_order=3, gamma=0.99, gauss_std=1.5, gpu=0, hyper='default', importance_clip=15.0, inc=1, log_output='term_file', max_episodes=1000, max_inner=30, max_steps=500, optim='rmsprop', oracle=-1, raw_basis=True, restore=False, save_count=100, save_model=True, seed=2, speed=0, state_lr=0.001, summary=True, swarm=False, timestamp='5|30|14:27:1')
Actions space: 5 :: State space: 1
State Low: tensor([0.]) :: State High: tensor([1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([5, 1])), ('fc1.bias', torch.Size([5]))]
0 :: Rewards -0.729 :: steps: 0.10 :: Time: 0.000(0.00053/step) :: Entropy : 0.000 :: Grads : [[], []]
10 :: Rewards -6.046 :: steps: 1.00 :: Time: 0.011(0.01109/step) :: Entropy : 0.000 :: Grads : [[0.36272115, 0.36272115], []]
20 :: Rewards -9.090 :: steps: 1.00 :: Time: 0.013(0.01258/step) :: Entropy : 0.000 :: Grads : [[0.15044883, 0.15044883], []]
30 :: Rewards -13.760 :: steps: 1.00 :: Time: 0.013(0.01289/step) :: Entropy : 0.000 :: Grads : [[0.08902838, 0.08902837], []]
40 :: Rewards -18.049 :: steps: 1.00 :: Time: 0.012(0.01238/step) :: Entropy : 0.000 :: Grads : [[0.11924424, 0.11924422], []]
